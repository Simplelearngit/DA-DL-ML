{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO25h2dB5a0jRGH3bOgNj2G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"CLPbAeLiWWhR","executionInfo":{"status":"error","timestamp":1683784176727,"user_tz":-330,"elapsed":14,"user":{"displayName":"26. Nilam Kolekar","userId":"02147768789417748747"}},"outputId":"22372732-c933-4eea-a960-b34aed4f1b8e"},"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-b4a0cd255c16>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    for file in os.listdir(directory + subdir): img_path =directory + subdir + '/' + file img =cv2.imread(img_path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 13\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","# Load the pre-trained VGG16 model\n","model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\n","# Define the dataset path and image dimensions\n","dataset_path = 'path/to/dataset/'\n","img_height, img_width = 224, 224\n","# Extract features from the dataset using the VGG16 model def\n","extract_features(directory);\n","features = {}\n","for subdir in os.listdir(directory):\n","for file in os.listdir(directory + subdir): img_path =directory + subdir + '/' + file img =cv2.imread(img_path)\n","img = cv2.resize(img, (img_height, img_width)) img =np.expand_dims(img, axis=0) features[file] =model.predict(img)[0]\n","return features\n","# Load the dataset and extract features\n","features = extract_features(dataset_path)\n","# Split the dataset into training and testing sets\n","train_features, train_labels, test_features, test_labels = [], [], [], [] for key\n","in features:\n","if 'train' in key:\n","train_features.append(features[key])\n","train_labels.append(key.split('_')[0])\n","else:\n","test_features.append(features[key])\n","test_labels.append(key.split('_')[0])\n","# Convert labels to one-hot encoding unique_labels = list(set(train_labels))\n","label_map = {label: i for i, label in enumerate(unique_labels)}\n","train_labels = [label_map[label] for label in train_labels] test_labels =\n","[label_map[label] for label in test_labels]\n","train_labels = tf.keras.utils.to_categorical(train_labels, len(unique_labels))\n","test_labels = tf.keras.utils.to_categorical(test_labels, len(unique_labels))\n","#Define the fully connected neural network\n","model = tf.keras.models.Sequential([\n","tf.keras.layers.Dense(512, activation='relu', input_dim=7*7*512),\n","tf.keras.layers.Dropout(0.5),\n","tf.keras.layers.Dense(len(unique_labels), activation='softmax')\n","])\n","# Compile the model\n","model.compile(loss='categorical_crossentropy',\n","optimizer='adam',\n","metrics=['accuracy'])\n","#Train the model\n","model.fit(np.array(train_features), np.array(train_labels),\n","batch_size=32,\n","epochs=10,\n","validation_data=(np.array(test_features), np.array(test_labels)))\n","# Evaluate the model\n","loss, accuracy = model.evaluate(np.array(test_features), np.array(test_labels))\n","print('Test Accuracy:', accuracy)"]}]}